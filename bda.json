{
    "big-data": "Big Data refers to extremely large, complex, and fast-growing datasets that cannot be efficiently stored, processed, or analyzed using traditional data processing tools and databases./nBig Data is characterized by the 5 V’s:/n1.Volume – Massive amount of data (terabytes, petabytes, exabytes) generated from sources like social media, sensors, transactions, etc./n2.Velocity – High speed at which data is generated, collected, and processed (real-time or near real-time)./n3.Variety – Different types of data such as structured (tables), semi-structured (JSON, XML), and unstructured (text, images, videos)./n4.Veracity – Uncertainty and quality issues in data, including noise, inconsistency, and incompleteness./n5.Value – The useful insights and business benefits extracted from data after analysis./nBig Data requires distributed computing frameworks like Hadoop and Spark to store and process data across multiple machines, enabling scalable and fault-tolerant data analysis.",
    "applications-of-big-data": "Applications of Big Data include the following major areas:/n1.Healthcare – Used for disease prediction, patient monitoring, personalized treatment, medical image analysis, and drug discovery./n2.Business and Marketing – Customer behavior analysis, targeted advertising, recommendation systems, and sales forecasting./n3.Finance and Banking – Fraud detection, risk management, credit scoring, and algorithmic trading./n4.Social Media Analytics – Sentiment analysis, trend detection, and user engagement analysis from large-scale social data./n5.E-commerce – Product recommendations, dynamic pricing, inventory management, and customer personalization./n6.Telecommunications – Network optimization, churn prediction, and real-time traffic analysis./n7.Smart Cities and IoT – Traffic management, energy optimization, smart surveillance, and environmental monitoring./nBig Data enables data-driven decision making, improved efficiency, cost reduction, and innovation across industries.",
    "aws-academy": "AWS Academy is an educational program by Amazon Web Services (AWS) that provides free cloud computing courses and hands-on labs to students and educators to build skills in cloud computing, Big Data, and data analytics./n1.It offers industry-aligned curriculum designed by AWS experts./n2.Provides hands-on practice using real AWS services like EC2, S3, RDS, EMR, and IAM./n3.Helps students understand cloud-based Big Data processing and storage./n4.Supports learning of scalable, distributed computing environments./n5.Prepares students for AWS certifications and cloud-related careers./n6.Enables institutions to integrate cloud learning into academic courses./n7.Promotes cost-effective, secure, and flexible cloud solutions for real-world applications./nAWS Academy bridges the gap between theoretical knowledge and practical cloud implementation in Big Data Analytics.",
    "aws-connection": "AWS Connection refers to the process of accessing and interacting with Amazon Web Services resources securely for cloud and Big Data operations./n1.AWS connection starts by creating an AWS account and logging in through the AWS Management Console./n2.IAM (Identity and Access Management) is used to create users, roles, and permissions for secure access./n3.Users can connect to AWS services via AWS Console (web UI), AWS CLI, or SDKs./n4.For compute services like EC2, connection is established using SSH (Linux) or RDP (Windows)./n5.Access Key ID and Secret Access Key are used for programmatic access through CLI and SDKs./n6.Network connections are secured using VPC, security groups, and key pairs./n7.Proper AWS connection enables users to deploy, manage, and analyze Big Data services like S3, EMR, and Redshift./nAWS connection ensures secure, controlled, and scalable access to cloud-based Big Data resources.",
    "aws-basic-operations": "AWS Basic Operations refer to the fundamental tasks performed to manage and use AWS cloud resources./n1.Account and IAM Management – Creating users, roles, policies, and assigning permissions for secure access./n2.Launching EC2 Instances – Creating, starting, stopping, rebooting, and terminating virtual servers./n3.Storage Operations (S3) – Creating buckets, uploading/downloading data, setting permissions, and managing lifecycle rules./n4.Database Operations – Creating and managing databases using services like RDS and DynamoDB./n5.Networking Operations – Configuring VPC, subnets, security groups, and elastic IPs./n6.Monitoring and Logging – Using CloudWatch to monitor performance and logs of AWS resources./n7.Scaling and Cost Management – Auto Scaling resources and tracking usage and costs using AWS Billing and Cost Explorer./nThese basic operations enable efficient deployment, monitoring, and management of Big Data and cloud applications on AWS.",
    "traditional-data-warehousing": "Traditional Data Warehousing is a centralized approach used to store, manage, and analyze structured data collected from multiple operational databases for decision making./n1.Data is stored in a central data warehouse using relational database systems (RDBMS)./n2.Data follows a schema-on-write approach, where structure is defined before data is loaded./n3.ETL process (Extract, Transform, Load) is used to clean and integrate data from different sources./n.4.Data is organized using star schema or snowflake schema for efficient querying./n5.Supports OLAP (Online Analytical Processing) for reporting and business intelligence./n6.Handles mainly structured data and has limited support for semi-structured or unstructured data./n7.Scalability is vertical (scale-up) and can be expensive for very large datasets./n Traditional data warehousing is effective for historical analysis and structured reporting, but is less suitable for Big Data environments.",
    "distributed-computing-environment": "Distributed Computing Environment is a system in which multiple independent computers (nodes) work together as a single system to process large-scale data and applications./nKey points:/nComputing tasks are divided into smaller sub-tasks and distributed across multiple nodes./nNodes communicate over a network to coordinate processing and data sharing./nProvides parallel processing, which improves performance and reduces execution time./nEnsures scalability by adding more nodes instead of upgrading a single machine./nSupports fault tolerance, where failure of one node does not stop the entire system./nEnables resource sharing such as CPU, memory, and storage across nodes./nForms the foundation of Big Data frameworks like Hadoop and Spark./nDistributed computing environments are essential for efficient storage and processing of Big Data.",
    "hadoop": "Hadoop is an open-source distributed framework used to store and process large volumes of data across clusters of commodity hardware./nKey points:/nHadoop follows a distributed computing model for Big Data processing./nIt consists of core components: HDFS, YARN, MapReduce, and Hadoop Common./nHDFS (Hadoop Distributed File System) stores data in a distributed and fault-tolerant manner./nMapReduce processes data in parallel using map and reduce functions./nYARN manages cluster resources and job scheduling./nProvides high scalability by adding more nodes to the cluster./nEnsures fault tolerance through data replication and automatic recovery./nHadoop enables cost-effective, reliable, and scalable Big Data analytics.",
    "hadoop-ecosystem": "Hadoop Ecosystem refers to the collection of tools and frameworks that work with Hadoop to store, process, manage, and analyze Big Data efficiently./nKey components:/nHDFS – Distributed storage system for large datasets./nYARN – Resource management and job scheduling in Hadoop clusters./nMapReduce – Programming model for parallel data processing./nHive – Data warehousing tool that provides SQL-like querying (HiveQL)./nPig – High-level scripting language for data processing./nHBase – NoSQL column-oriented database for real-time read/write access./nSqoop & Flume – Tools for data ingestion from RDBMS and streaming sources./nThe Hadoop ecosystem provides a complete platform for Big Data storage, processing, analysis, and management.",
    "hdfs-architecture": "HDFS Architecture defines how data is stored and managed in a distributed manner within the Hadoop Distributed File System./nKey components and working:/nNameNode – Master node that manages metadata such as file names, directory structure, and block locations./nDataNode – Worker nodes that store actual data blocks and handle read/write requests./nSecondary NameNode – Periodically merges metadata and edit logs to reduce NameNode overhead (not a backup)./nFiles are divided into fixed-size blocks (default 128 MB)./nEach block is replicated across multiple DataNodes for fault tolerance (default replication factor is 3)./nHDFS follows a master–slave architecture for efficient management./nProvides high reliability, scalability, and fault tolerance for Big Data storage./nHDFS architecture enables efficient and secure storage of massive datasets across distributed systems.",
    "hdfs-commands": "HDFS Commands are used to interact with and manage files and directories in the Hadoop Distributed File System./nCommon HDFS commands:/nhdfs dfs -ls – Lists files and directories in HDFS./nhdfs dfs -mkdir – Creates a directory in HDFS./nhdfs dfs -put – Uploads files from local system to HDFS./nhdfs dfs -get – Downloads files from HDFS to local system./nhdfs dfs -rm – Deletes files or directories from HDFS./nhdfs dfs -du – Displays disk usage of files and directories./nhdfs dfs -df – Shows available and used space in HDFS./nHDFS commands help users store, retrieve, and manage Big Data efficiently in a distributed environment.",
    "map-reduce": "MapReduce is a distributed programming model used in Hadoop to process large datasets in parallel across a cluster./nKey points:/nMapReduce works in two main phases: Map and Reduce./nMap phase processes input data and converts it into key–value pairs./nShuffle and Sort phase groups all values belonging to the same key./nReduce phase aggregates and processes the grouped data to produce final output./nProcessing is done in parallel across multiple nodes, improving performance./nProvides fault tolerance, as failed tasks are automatically re-executed./nSuitable for batch processing of large-scale data./nMapReduce enables scalable, reliable, and efficient Big Data processing in Hadoop.",
    "hive": "Hive is a data warehousing and query tool in the Hadoop ecosystem that allows users to analyze large datasets stored in HDFS using SQL-like queries./nKey points:/nHive provides a high-level query language called HiveQL, similar to SQL. It follows a schema-on-read approach, defining structure at query time./nHive converts queries into MapReduce or Spark jobs for execution./nUsed mainly for batch processing and analytical queries./nSupports structured and semi-structured data./nUses metastore to store metadata about tables and schemas./nNot suitable for real-time querying due to higher query latency./nHive simplifies Big Data analysis by enabling SQL-based querying on Hadoop.",
    "hive-architecture": "Hive Architecture describes the components involved in querying and processing data stored in HDFS using Hive./nKey components:/nUser Interface (UI) – Provides access through CLI, JDBC/ODBC, or web interfaces./nDriver – Receives HiveQL queries, manages query lifecycle, and coordinates execution./nCompiler – Parses, validates, and converts HiveQL queries into an execution plan./nMetastore – Stores metadata such as table definitions, schemas, and locations./nExecution Engine – Executes the query plan using MapReduce or Spark./nHDFS – Underlying storage layer where actual data is stored./nResource Management (YARN) – Allocates cluster resources for query execution./nHive architecture enables efficient, scalable, and SQL-like analytics on Big Data.",
    "hql": "HQL (Hive Query Language) is a SQL-like query language used in Hive to query and manage large datasets stored in HDFS./nKey points:/nHQL syntax is similar to SQL, making it easy for users to learn./nUsed to perform data definition, data manipulation, and data querying operations./nSupports commands like CREATE, LOAD, INSERT, SELECT, DROP./nWorks on schema-on-read, where structure is applied at query time./nQueries are internally converted into MapReduce or Spark jobs./nSupports partitioning and bucketing for improved query performance./nMainly used for batch analytics, not real-time processing./nHQL simplifies Big Data analysis by providing a declarative query interface over Hadoop.",
    "bucketing-hive": "Bucketing in Hive is a data organization technique used to divide a table into fixed number of smaller parts (buckets) based on a hash function of a column./nKey points:/nData is distributed into buckets using hash(column) mod number_of_buckets./nBucketing improves query performance by reducing data scanned./nUseful for efficient joins and sampling operations./nBuckets are stored as separate files in HDFS./nRequires the number of buckets to be defined at table creation time./nWorks well when combined with partitioning./nEnables better data management and parallel processing./nBucketing in Hive helps optimize large-scale data processing and analytics.",
    "partitioning-hive": "Partitioning in Hive is a technique used to divide a table into smaller, manageable parts called partitions based on the values of one or more columns./nKey points:/nData is organized into separate directories in HDFS based on partition columns./nImproves query performance by scanning only relevant partitions./nCommonly used partition columns include date, country, region, or category./nReduces I/O operations during query execution./nPartition columns are not stored in the data files, but derived from directory structure./nSupports static and dynamic partitioning./nEspecially useful for large datasets and time-based data analysis./nPartitioning in Hive enables efficient, scalable, and faster querying of Big Data.",
    "importing-exporting-data-to-from-hive-using-sqoop": "Importing and Exporting Data to/from Hive using Sqoop enables data transfer between relational databases and Hive tables in a Hadoop environment./nKey points:/nSqoop is a tool designed to transfer bulk data between RDBMS and Hadoop/Hive./nImporting moves data from an RDBMS (MySQL, Oracle, etc.) into Hive tables./nSqoop import automatically creates Hive tables and loads data into HDFS./nExporting transfers data from Hive/HDFS back to RDBMS tables./nUses parallel processing to improve data transfer speed./nSupports incremental imports for updating Hive data./nEnsures efficient integration of structured data with Big Data analytics./nSqoop acts as a bridge between traditional databases and Hive-based Big Data systems.",
    "file-formats-orc-parquet-avro": "ORC, Parquet, and Avro are optimized file formats used in Big Data systems for efficient storage and processing./nORC (Optimized Row Columnar): Columnar storage format optimized for Hive./nProvides high compression and fast query performance./nSupports predicate pushdown and indexing./nBest for analytical queries./nParquet:/n5. Columnar storage format widely used with Spark and Hive./n6. Enables efficient compression and faster reads for selected columns./nAvro:/n7. Row-based format mainly used for data serialization and data exchange, supports schema evolution and is suitable for streaming./nThese formats improve storage efficiency, performance, and interoperability in Big Data analytics.",
    "flume": "Flume is a distributed, reliable data ingestion tool used to collect, aggregate, and transfer large volumes of streaming data into Hadoop./nKey points:/n1. Flume is mainly used for ingesting real-time data such as logs and event data./n2. It has a simple architecture consisting of Source, Channel, and Sink./n3. Source collects data from external systems./n4. Channel temporarily stores data in memory or disk./n5. Sink delivers data to destinations like HDFS or Hive./n6. Provides fault tolerance through transactional mechanisms./n7. Supports scalability and reliability for continuous data flow./nFlume is widely used for log data ingestion in Big Data analytics systems.",
    "nosql": "NoSQL refers to a class of non-relational databases designed to store and manage large volumes of distributed and unstructured data./nKey points:/n1. NoSQL databases do not use fixed table schemas like RDBMS./n2. They support schema-less or flexible data models./n3. Designed for horizontal scalability across multiple nodes./n4. Provide high availability and fault tolerance./n5. Handle structured, semi-structured, and unstructured data efficiently./n6. Types include key–value, document, column-family, and graph databases./n7. Widely used in Big Data, real-time analytics, and web-scale applications./nNoSQL databases overcome the limitations of traditional databases in Big Data environments.",
    "cap-theorem": "CAP Theorem states that a distributed system cannot simultaneously guarantee all three properties: Consistency, Availability, and Partition Tolerance./nKey points:/n1. Consistency (C) – Every read receives the most recent write or an error./n2. Availability (A) – Every request receives a response, without guarantee of latest data./n3. Partition Tolerance (P) – System continues to operate despite network failures./n4. In the presence of a network partition, a system must choose between Consistency or Availability./n5. CP systems prioritize consistency over availability (e.g., HBase)./n6. AP systems prioritize availability over consistency (e.g., Cassandra)./n7. CAP Theorem guides the design and selection of NoSQL databases./nCAP Theorem explains the trade-offs in distributed Big Data systems.",
    "nosql-database-types": "NoSQL Database Types refer to the different models used to store and manage unstructured or semi-structured Big Data./n1. Key–Value Stores – Data is stored as key-value pairs; ideal for fast lookups./n   * Example: Redis, Riak./n2. Document Stores – Data is stored as documents (JSON, XML) with flexible schema./n   * Example: MongoDB, CouchDB./n3. Column-Family Stores – Data stored in columns instead of rows; optimized for analytical queries./n   * Example: Cassandra, HBase./n4. Graph Databases – Data stored as nodes and edges to represent relationships./n   * Example: Neo4j, JanusGraph./n5. Time-Series Databases (optional subtype) – Optimized for storing time-stamped data./n   * Example: InfluxDB, OpenTSDB./nThese types provide scalable, flexible, and high-performance storage solutions for Big Data applications.",
    "mongo-db": "MongoDB is a NoSQL document-oriented database designed to store, manage, and query large volumes of unstructured or semi-structured data./nKey points:/n1. Stores data as JSON-like documents (BSON) with flexible schemas./n2. Supports dynamic schema, allowing addition of fields without altering existing documents./n3. Provides high scalability via sharding (horizontal scaling across nodes)./n4. Ensures high availability via replica sets (data replication across nodes)./n5. Supports rich query capabilities, indexing, and aggregation framework./n6. Ideal for real-time analytics, web applications, and Big Data projects./n7. Integrates easily with Hadoop and Spark for large-scale data processing./nMongoDB is widely used in Big Data for flexible, distributed, and high-performance data management.",
    "mongodb-features": "MongoDB Features highlight why it’s widely used for Big Data and NoSQL applications:/n1. Document-Oriented Storage – Stores data as BSON (Binary JSON) documents with flexible schema./n2. Dynamic Schema – Allows adding or modifying fields without affecting existing documents./n3. High Scalability – Supports horizontal scaling (sharding) across multiple servers./n4. High Availability – Uses replica sets to replicate data across nodes for fault tolerance./n5. Rich Query Language – Supports complex queries, filtering, and indexing./n6. Aggregation Framework – Enables data aggregation, transformation, and analytics./n7. Indexing Support – Provides multiple index types for faster data retrieval./nThese features make MongoDB suitable for flexible, distributed, and efficient Big Data solutions.",
    "mongodb-collections": "MongoDB Collections are the containers for documents in a MongoDB database, functioning similarly to tables in relational databases (RDBMS)./n1. Grouping of Documents – A collection holds a set of documents, which are the basic units of data in MongoDB./n2. Schemaless / Dynamic Schema – Unlike RDBMS tables, collections do not enforce a rigid schema. Documents within the same collection can have different fields and structures./n3. Implicit Creation – A collection is automatically created when the first document is inserted into it./n4. Explicit Creation – Users can explicitly create collections using 'db.createCollection()' to set specific options like validation rules or storage limits./n5. Capped Collections – MongoDB supports fixed-size collections called capped collections, which maintain insertion order and automatically overwrite the oldest documents when the size limit is reached./n6. Indexing – Indexes are defined at the collection level to improve query performance./n7. Namespace – Each collection has a unique namespace (database_name.collection_name) and exists within a single database./nMongoDB collections provide the structural flexibility required for handling diverse and evolving Big Data datasets.",
    "mongodb-documents": "MongoDB Documents are the basic units of data in MongoDB, analogous to rows in a relational database (RDBMS), but stored in a flexible format./n1. BSON Format – Documents are stored in BSON (Binary JSON), a binary representation that supports more data types than standard JSON./n2. Key-Value Pairs – A document consists of field and value pairs, where the field is a string and the value can be any supported data type./n3. Unique Identifier (_id) – Every document must have a unique primary key field named '_id', which is automatically generated (ObjectId) if not provided./n4. Flexible Schema – Documents in the same collection do not need to have the same set of fields or structure, allowing for easy schema evolution./n5. Embedded Documents – MongoDB allows nesting documents within documents (sub-documents) to model complex relationships efficiently./n6. Data Types – Supports a rich set of data types including String, Integer, Boolean, Arrays, Date, and Binary data./n7. Document Size Limit – The maximum size of a single BSON document is 16 megabytes to ensure efficient memory usage and network transmission./nMongoDB documents provide a powerful and flexible way to represent complex, hierarchical data structures essential for Big Data applications.",
    "mongodb-operations": "MongoDB Operations primarily revolve around CRUD (Create, Read, Update, Delete) actions and aggregation tasks to manage data effectively./n1. Create Operations – Used to insert new documents into a collection. Commands include 'insertOne()' for single documents and 'insertMany()' for batch insertions./n2. Read Operations – Retrieve data using 'find()' or 'findOne()'. Queries can include filters, projections, and sorting to narrow down results./n3. Update Operations – Modify existing documents. 'updateOne()' changes the first match, while 'updateMany()' affects all matching documents. Operators like '$set' and '$inc' are used./n4. Delete Operations – Remove documents permanently. 'deleteOne()' removes the first match, and 'deleteMany()' removes all documents matching a filter./n5. Aggregation Operations – The 'aggregate()' method uses a pipeline (e.g., $match, $group, $sort) for complex data processing and analytics, similar to SQL GROUP BY./n6. Bulk Write Operations – Allows execution of multiple insert, update, and delete operations in a single batch for better performance./n7. Index Management – Operations like 'createIndex()' build indexes to significantly speed up query performance./nThese operations provide the necessary functional primitives for building scalable Big Data applications on MongoDB.",
    "pymongo": "PyMongo is the official Python driver for MongoDB, enabling Python applications to interact seamlessly with MongoDB databases./n1. Connection Handling – Uses 'MongoClient' to establish connections to MongoDB instances, supporting both local and URI-based connections./n2. Native Data Types – Automatically maps MongoDB BSON documents to Python dictionaries, allowing developers to work with familiar data structures./n3. CRUD Support – Provides methods like 'insert_one()', 'find()', 'update_many()', and 'delete_one()' to perform standard database operations./n4. Aggregation Pipeline – Fully supports MongoDB's aggregation framework, allowing complex data analysis pipelines to be constructed using Python lists./n5. Index Management – Allows creating and managing indexes programmatically using 'create_index()' to optimize query performance./n6. High Performance – Written in C extensions for speed (where available) and supports connection pooling for efficient resource usage in multi-threaded applications./n7. GridFS Support – Includes support for GridFS, enabling storage and retrieval of files exceeding the 16MB document size limit./nPyMongo is the standard tool for integrating Python's data analysis capabilities with MongoDB's storage.",
    "python-driver-connect-to-mongodb": "Connecting a Python application to MongoDB is primarily done using the PyMongo driver to establish a robust and secure client-server link./n1. Installation – The first step involves installing the PyMongo library using 'pip install pymongo'./n2. MongoClient Class – The central component for initiating a connection is the 'pymongo.MongoClient' class, which represents a connection pool to a running MongoDB instance./n3. Local Connection – For a local MongoDB server, a basic connection can be established by instantiating 'MongoClient()' or specifying 'MongoClient('localhost', 27017)'./n4. Remote/Atlas Connection – To connect to a remote MongoDB server or a cloud service like MongoDB Atlas, a connection URI is typically used: 'MongoClient('mongodb://user:pass@host:port/dbname')'./n5. Database and Collection Access – Once connected, databases and collections are accessed via dictionary-like syntax or attribute access on the MongoClient object (e.g., 'client.mydatabase.mycollection')./n6. Connection Pooling – PyMongo automatically manages a pool of connections, reusing them for subsequent requests, which significantly improves performance and scalability./n7. Secure Connections (SSL/TLS) – Secure communication with MongoDB can be enabled by specifying SSL/TLS options in the MongoClient constructor, essential for production environments./nThis connection mechanism is foundational for performing any data operations from Python on MongoDB.",
    "crud-operations-using-pymongo": "CRUD Operations using PyMongo allow Python applications to perform Create, Read, Update, and Delete actions on MongoDB collections using the pymongo driver./n1. Create (Insert) – Data is inserted as Python dictionaries. 'insert_one(doc)' adds a single document, returning a result with the '_id', while 'insert_many(list)' adds multiple documents in a batch./n2. Read (Query) – 'find_one(filter)' retrieves a single document matching the criteria. 'find(filter)' returns an iterable Cursor for all matching documents. Filters are defined using dictionary syntax./n3. Update (Modify) – 'update_one(filter, update)' modifies the first matching document. 'update_many(filter, update)' modifies all matches. Updates require atomic operators like '$set' or '$inc' to define changes./n4. Delete (Remove) – 'delete_one(filter)' removes the first matching document, and 'delete_many(filter)' removes all documents that match the filter criteria./n5. ObjectIds – When querying by the primary key '_id', strings must often be converted to 'ObjectId' instances using 'bson.objectid.ObjectId'./n6. Result Objects – Write operations return result objects (e.g., InsertOneResult, UpdateResult) containing useful metadata like 'matched_count', 'modified_count', and 'deleted_count'./n7. Type Mapping – PyMongo automatically handles the conversion between BSON types in MongoDB and native Python types (dicts, lists, datetimes) during these operations./nThese operations enable full data manipulation capabilities directly from Python code.",
    "sharding": "Sharding is a method for distributing data across multiple machines to support deployments with very large datasets and high throughput operations./n1. Horizontal Scaling – Unlike vertical scaling (adding more power to a single server), sharding adds more servers (nodes) to the cluster to handle increased load./n2. Shards – Specific subsets of data are stored on shards. Each shard is a separate database instance (or replica set) that holds a portion of the overall data./n3. Config Servers – These store metadata and configuration settings for the cluster, mapping chunks of data to specific shards./n4. Mongos (Query Router) – Acts as an interface between client applications and the sharded cluster, routing queries to the appropriate shard(s) based on the metadata./n5. Shard Key – A specific field determines how data is distributed. Choosing an effective shard key is crucial for ensuring even distribution and avoiding 'hot spots'./n6. Chunks – Data is divided into smaller units called chunks. The balancer migrates these chunks across shards to ensure even load distribution./n7. Benefits – Provides virtually unlimited storage capacity, increased read/write throughput, and high availability for Big Data applications./nSharding is the primary mechanism for scaling MongoDB and other Big Data systems beyond the limits of a single server.",
    "spark": "Apache Spark is a unified analytics engine for large-scale data processing that is significantly faster than traditional Hadoop MapReduce due to its in-memory computing capabilities./n1. In-Memory Processing – Stores intermediate data in RAM (Random Access Memory) rather than writing to disk after each step, making it up to 100x faster than MapReduce./n2. Unified Engine – Supports multiple workloads including batch processing, streaming, SQL queries, machine learning, and graph processing within a single framework./n3. RDDs (Resilient Distributed Datasets) – The fundamental data structure of Spark, which is an immutable, distributed collection of objects that can be processed in parallel./n4. Multi-Language Support – Provides high-level APIs in Java, Scala, Python (PySpark), and R, making it accessible to a wide range of developers./n5. Lazy Evaluation – Transformations on RDDs are not executed immediately but recorded as a lineage graph; execution only happens when an action (like count or collect) is triggered, optimizing performance./n6. Ecosystem Integration – Runs on Hadoop YARN, Apache Mesos, Kubernetes, or standalone, and can access data in HDFS, Cassandra, HBase, and S3./n7. Fault Tolerance – Automatically recovers lost data by recomputing partitions based on the lineage graph tracked in RDDs./nSpark has become the de facto standard for fast, flexible, and sophisticated Big Data analytics.",
    "spark-architecture": "Spark Architecture follows a master-slave model consisting of a Driver Program, a Cluster Manager, and multiple Worker Nodes to execute parallel operations./n1. Driver Program – The control node that runs the user's main() function and creates the SparkContext (or SparkSession). It converts the user program into tasks./n2. SparkContext – The entry point to Spark functionality; it connects to the cluster manager to allocate resources and coordinates job execution./n3. Cluster Manager – An external service (like YARN, Mesos, or Standalone) responsible for acquiring resources on the worker nodes./n4. Worker Nodes – Slave nodes in the cluster that run the application code. Each worker node has available CPU, memory, and storage resources./n5. Executors – Processes launched on worker nodes to run individual tasks. They store data in memory (cache) and execute computation logic./n6. Tasks – The smallest unit of work sent to an executor. The driver breaks down a job into stages and then into tasks to be distributed across executors./n7. DAG Scheduler – Converts the logical execution plan (RDD lineage) into a physical execution plan of stages and tasks, optimizing the workflow./nThis architecture enables Spark to efficiently distribute processing, handle faults, and manage resources dynamically across a cluster.",
    "pyspark": "PySpark is the Python API for Apache Spark, enabling Python developers to harness Spark's powerful distributed data processing capabilities./n1. Python-Spark Integration – PySpark uses Py4J to allow Python programs to dynamically access objects in the JVM (Java Virtual Machine), where Spark's core engine runs./n2. SparkSession – The primary entry point for PySpark applications, allowing interaction with Spark functionalities like creating DataFrames, executing SQL queries, and managing configurations./n3. RDDs and DataFrames – Supports both Resilient Distributed Datasets (RDDs) for low-level transformations and DataFrames/Datasets for structured data processing with optimized performance./n4. Spark SQL – Enables SQL queries and DataFrame operations, allowing integration with existing SQL tools and efficient processing of structured data./n5. MLlib Integration – Provides access to Spark's machine learning library (MLlib), allowing for scalable implementation of various ML algorithms on large datasets./n6. Spark Streaming – Facilitates real-time data processing by integrating with streaming sources like Kafka and Flume, enabling analysis of live data streams./n7. Ecosystem Compatibility – PySpark applications can run on various cluster managers (YARN, Mesos, Kubernetes) and process data from diverse sources like HDFS, S3, and Cassandra./nPySpark combines Python's simplicity and rich ecosystem with Spark's speed and scalability, making it a popular choice for Big Data analytics and machine learning.",
    "databricks": "Databricks is a unified data analytics platform built on Apache Spark that provides a collaborative workspace for data science, machine learning, and data engineering./n1. Lakehouse Architecture – Databricks pioneered the Lakehouse architecture, which combines the reliability of data warehouses with the flexibility and scalability of data lakes./n2. Collaborative Workspace – Offers interactive notebooks, dashboards, and job scheduling tools that enable data teams to collaborate on data projects in real-time./n3. Optimized Spark Runtime – Includes an optimized version of Apache Spark (Databricks Runtime) that delivers significant performance improvements over standard Spark distributions./n4. Delta Lake – An open-source storage layer that brings ACID (Atomicity, Consistency, Isolation, Durability) transactions, schema enforcement, and unified streaming and batch data processing to data lakes./n5. MLflow Integration – Tightly integrated with MLflow, an open-source platform for managing the end-to-end machine learning lifecycle, from experimentation to deployment./n6. Auto-Scaling Clusters – Automatically scales Spark clusters up and down based on workload, optimizing resource utilization and cost efficiency./n7. Multiple Language Support – Supports Python, Scala, R, and SQL, allowing data professionals to work in their preferred programming language./nDatabricks simplifies Big Data analytics and machine learning by providing a fully managed, high-performance platform that streamlines the entire data pipeline.",
    "spark-sql": "Spark SQL is a Spark module for structured data processing that provides a programming interface for working with structured data, allowing users to query data using SQL or a DataFrame API./n1. Unified Data Access – Unifies traditional relational database capabilities with Spark's powerful functional programming API, enabling seamless integration of SQL queries within Spark programs./n2. DataFrames – The primary abstraction in Spark SQL, representing a distributed collection of data organized into named columns, similar to a table in a relational database or a DataFrame in R/Python./n3. Catalyst Optimizer – Features a powerful optimization engine that automatically analyzes and optimizes Spark SQL queries, significantly improving performance by rewriting query plans./n4. Tungsten Execution Engine – Leverages an advanced execution engine that performs whole-stage code generation and aggressive memory management for highly efficient data processing./n5. Diverse Data Sources – Can read and write data from various formats, including Parquet, JSON, CSV, ORC, JDBC connections, and existing Hive tables, providing broad data integration capabilities./n6. SQL and DataFrame API – Supports both standard ANSI SQL queries and a DataFrame API (in Python, Scala, Java, and R) for programmatic data manipulation, offering flexibility for developers./n7. Use Cases – Widely used for ETL (Extract, Transform, Load) processes, ad-hoc analysis, reporting, and integrating machine learning workflows with structured data./nSpark SQL is a cornerstone of the Spark ecosystem for efficient and scalable analysis of structured and semi-structured Big Data.",
    "spark-sql-as-an-etl-tool": "Spark SQL is a robust and scalable tool for ETL (Extract, Transform, Load) processes, enabling efficient data pipelines for Big Data./n1. Extraction from Diverse Sources – Spark SQL can seamlessly read data from a wide variety of formats (JSON, Parquet, ORC, CSV, Text) and storage systems (HDFS, S3, JDBC databases, Hive)./n2. Powerful Transformations – It provides a rich set of APIs (DataFrame/Dataset) and SQL support for complex data manipulations like filtering, aggregations, joins, and window functions./n3. Optimized Performance – The Catalyst Optimizer and Tungsten execution engine ensure that ETL jobs run efficiently by optimizing query plans and memory usage./n4. Scalability – As a distributed engine, Spark SQL scales horizontally to process petabytes of data across a cluster, far exceeding the capabilities of single-node ETL tools./n5. Schema Handling – It offers features like schema inference and schema evolution, making it adaptable to changing data structures often found in data lakes./n6. Integration with Ecosystem – ETL pipelines can easily integrate with Spark Streaming for real-time processing and MLlib for machine learning feature engineering within the same application./n7. Loading and Storage – Processed data can be written back to diverse destinations in optimized formats (like partitioned Parquet files) to support downstream analytics and reporting./nSpark SQL thus provides a unified, high-performance framework for building modern, scalable data engineering pipelines.",
    "spark-sql-performance-tuning": "Spark SQL Performance Tuning involves optimizing configurations and code to maximize the efficiency and speed of data processing jobs./n1. Caching and Persistence – Using 'cache()' or 'persist()' to store frequently accessed DataFrames in memory prevents re-computation of intermediate results./n2. Broadcast Joins – For joining a large table with a small table, using 'broadcast()' sends the small table to all nodes, avoiding expensive network shuffles./n3. Partition Tuning – Managing partitions using 'repartition()' (increasing parallelism) or 'coalesce()' (reducing partitions without shuffle) ensures optimal resource utilization./n4. Optimized File Formats – Using columnar formats like Parquet or ORC enables compression, column pruning, and predicate pushdown, drastically reducing I/O./n5. Shuffle Configuration – Tuning 'spark.sql.shuffle.partitions' (default is 200) to match the dataset size prevents small file issues or out-of-memory errors during joins and aggregations./n6. Catalyst Optimizer – Leveraging Spark's built-in optimizer which automatically rewrites query plans for better execution paths; 'explain()' can be used to analyze these plans./n7. Serialization – Switching to Kryo serialization or utilizing Tungsten's binary format reduces memory overhead and improves data transfer speeds./nThese techniques collectively reduce execution time and resource consumption in large-scale Spark SQL applications.",
    "spark-ml": "Spark ML (also known as MLlib) is Apache Spark's scalable machine learning library designed to make practical machine learning scalable and easy./n1. Unified API – It provides a high-level DataFrame-based API ('spark.ml') that is user-friendly and interoperable with Spark SQL and other components./n2. Comprehensive Algorithms – Includes a wide array of common ML algorithms for classification, regression, clustering, and collaborative filtering (e.g., Logistic Regression, K-Means, Random Forests)./n3. ML Pipelines – Inspired by scikit-learn, it offers a Pipeline API to chain together multiple stages (feature transformers and model estimators) into a single workflow./n4. Feature Engineering – Provides extensive tools for feature extraction, transformation, and selection (e.g., hashing, scaling, normalization, PCA) to prepare raw data for modeling./n5. Model Tuning – Includes utilities for hyperparameter tuning and model selection using cross-validation and grid search to find the best performing models./n6. Scalability – Built on top of Spark, it scales horizontally to process massive datasets distributed across a cluster, enabling training on data that fits nowhere near a single machine's memory./n7. Persistence – Supports saving and loading models and pipelines, allowing for easy deployment and reuse of trained models in production environments./nSpark ML creates a powerful, unified environment for end-to-end machine learning workflows on Big Data.",
    "spark-ml-pipeline": "Spark ML Pipeline is a workflow construction concept in Spark MLlib that represents a sequence of data processing and machine learning stages to simplify the creation and tuning of practical ML workflows./n1. Unified Workflow – It chains together multiple algorithms into a single pipeline, ensuring that the sequence of data preparation and modeling steps is consistent and reproducible./n2. Pipeline Stages – A pipeline consists of a sequence of stages, where each stage is either a Transformer or an Estimator./n3. Transformer – An abstraction that includes feature transformers and learned models. It implements a 'transform()' method to convert one DataFrame into another (e.g., adding a prediction column)./n4. Estimator – An abstraction for learning algorithms that fits or trains on data. It implements a 'fit()' method on a DataFrame to produce a Model (which is a Transformer)./n5. Parameter API – All stages use a common API for specifying parameters, allowing users to easily configure and tune different parts of the pipeline./n6. PipelineModel – When a Pipeline is fit on training data, it produces a PipelineModel, which is a Transformer that can be used to make predictions on new test data./n7. Cross-Validation – Pipelines seamlessly integrate with Spark's cross-validation and hyperparameter tuning tools, allowing the entire workflow to be optimized as a single unit./nSpark ML Pipelines standardize the process of building, tuning, and deploying complex machine learning applications.",
    "transformers": "Transformers in Spark ML are a key abstraction representing an algorithm that can transform one DataFrame into another DataFrame./n1. Core Functionality – A Transformer implements a method called 'transform()', which accepts a DataFrame and produces a new DataFrame, typically by appending one or more columns./n2. Feature Transformers – These are used for preparing data, such as converting text to vectors (Tokenizer), mapping strings to indices (StringIndexer), or normalizing numerical features (StandardScaler)./n3. Learned Models – In Spark ML, a trained model (e.g., a LogisticRegressionModel) is also a Transformer; it takes a DataFrame with features and adds a column with predictions./n4. Immutability – Since Spark DataFrames are immutable, Transformers do not modify the original data in place but return a new DataFrame with the applied transformations./n5. Pipeline Integration – Transformers are one of the two main stages in an ML Pipeline (alongside Estimators), allowing for seamless chaining of data processing and prediction steps./n6. Uniform API – They share a common parameter API, making it easy to configure different types of transformers using the same syntax./n7. Lazy Execution – Like other Spark operations, transformations defined by Transformers are lazily evaluated and only executed when an action creates a need for the result./nTransformers provide a consistent and scalable mechanism for feature engineering and model application in Big Data workflows.",
    "estimators": "Estimators in Spark ML are an abstraction that represents a learning algorithm or any algorithm that can be 'fit' on data to produce a Transformer./n1. Core Functionality – An Estimator implements a 'fit()' method, which takes a DataFrame (typically training data) and produces a Model, which is itself a Transformer./n2. Learning Algorithms – Examples include Logistic Regression, K-Means clustering, and Random Forest classifiers, all of which learn patterns from data./n3. Produces a Model (Transformer) – The output of an Estimator's 'fit()' method is always a Model. This Model is then used to 'transform()' new data, e.g., make predictions or apply transformations./n4. Pipeline Integration – Estimators are the other main component of an ML Pipeline, representing the training or fitting stage, typically following data pre-processing by Transformers./n5. Parameterization – Like Transformers, Estimators use a common API for setting parameters, enabling easy configuration and tuning of hyper-parameters./n6. Scalable Training – Being built on Spark, Estimators can be trained on massive, distributed datasets efficiently, leveraging Spark's cluster computing capabilities./n7. Cross-Validation & Tuning – Estimators are directly integrated with Spark's cross-validation and grid search tools, allowing for systematic hyperparameter optimization to find the best performing models./nEstimators are fundamental to building and training machine learning models within the structured and scalable environment of Spark ML Pipelines.",
    "spark-ml-component-flow": "Spark ML Component Flow describes the typical sequence of steps and component interactions for building a machine learning application using Spark MLlib./n1. Data Ingestion (DataFrame) – The process starts by loading raw data from a source (e.g., CSV, Parquet) into a DataFrame./n2. Feature Engineering (Transformers) – Raw data is passed through Transformers (like Tokenizer, VectorAssembler) to convert it into feature vectors suitable for ML algorithms./n3. Pipeline Construction – A Pipeline is defined to chain these Transformers and an Estimator into a single workflow object./n4. Model Training (Estimator) – The Pipeline (acting as an Estimator) calls 'fit()' on the training DataFrame. The Estimator learns from the data and produces a PipelineModel./n5. Model Application (Transformer) – The resulting PipelineModel is a Transformer. It calls 'transform()' on a test DataFrame to generate predictions./n6. Evaluation (Evaluator) – An Evaluator (e.g., BinaryClassificationEvaluator) compares the predictions against true labels to calculate metrics like accuracy or AUC./n7. Tuning (CrossValidator) – If needed, the flow wraps the Pipeline in a CrossValidator to automate hyperparameter tuning and model selection./nThis flow ensures a structured, scalable, and reproducible path from raw data to a deployed machine learning model.",
    "spark-ml-data-types": "Spark ML Data Types are crucial for defining the structure and ensuring compatibility of data used in machine learning workflows within Apache Spark./n1. Primitive Types – Spark ML leverages standard primitive data types for individual features or labels, including numeric types (Integer, Long, Double) and StringType, as part of DataFrame schemas./n2. Vector Types – The fundamental data type for representing features and predictions is 'Vector', which can be either 'DenseVector' (for most non-zero elements) or 'SparseVector' (for many zero elements), efficiently handling high-dimensional data./n3. LabeledPoint – This specialized type is used for supervised learning tasks, combining a 'label' (typically a DoubleType) and a 'features' Vector, representing a single training example./n4. Row Type – Data within Spark DataFrames is organized into 'Row' objects, where each row can contain a collection of different data types corresponding to the DataFrame's schema./n5. Schema Definition – DataFrame 'Schema' defines the names and types of columns, ensuring data consistency and enabling Spark's Catalyst Optimizer to perform optimizations./n6. Type Conversion & Casting – Spark ML provides functions and mechanisms for converting data between different types, essential for preparing raw data into a format suitable for algorithms (e.g., StringIndexer)./n7. Matrix Types – For advanced linear algebra operations, Spark ML also includes 'Matrix' types like 'DenseMatrix' and 'SparseMatrix', used in algorithms like Principal Component Analysis (PCA)./nThese data types underpin the efficiency, scalability, and flexibility of machine learning development in Spark.",
    "spark-ml-algorithms": "Spark ML Algorithms refers to the comprehensive suite of scalable machine learning algorithms provided by the Spark MLlib library for various analytical tasks./n1. Classification – Algorithms for predicting categorical labels, including Logistic Regression, Decision Trees, Random Forests, Gradient-Boosted Trees (GBTs), and Naive Bayes./n2. Regression – Algorithms for predicting continuous values, such as Linear Regression, Generalized Linear Regression (GLM), Decision Tree Regression, and Survival Regression./n3. Clustering – Unsupervised learning algorithms for grouping similar data points, including K-Means, Gaussian Mixture Models (GMM), Bisecting K-Means, and Latent Dirichlet Allocation (LDA)./n4. Collaborative Filtering – Algorithms for recommendation systems, primarily Alternating Least Squares (ALS) which is used for explicit or implicit feedback datasets./n5. Frequent Pattern Mining – Algorithms for finding frequent items or association rules in transactional data, such as FP-Growth and PrefixSpan./n6. Dimensionality Reduction – Techniques to reduce the number of features while preserving information, including Principal Component Analysis (PCA) and Singular Value Decomposition (SVD)./n7. Evaluation & Tuning – While not algorithms per se, MLlib includes specific evaluators (e.g., MulticlassClassificationEvaluator) and tools for cross-validation to assess and optimize these models./nThese algorithms are optimized for distributed computing, enabling them to process massive datasets efficiently across a Spark cluster.",
    "building-pipeline": "Building a Pipeline in Spark ML involves constructing a linear sequence of stages to automate the machine learning workflow, ensuring reproducibility and organization./n1. Define Stages – Identify the necessary steps, which typically include data preprocessing transformers (like Tokenizer, VectorAssembler) and a final learning algorithm or estimator (like LogisticRegression)./n2. Configure Transformers – Initialize each transformer with input and output column names to define how data flows and changes from one stage to the next./n3. Configure Estimator – Set up the machine learning algorithm with appropriate hyperparameters to train on the processed feature vectors./n4. Create Pipeline Object – Instantiate a 'Pipeline' class, passing the list of ordered stages to the 'stages' parameter (e.g., Pipeline(stages=[tokenizer, assembler, lr]))./n5. Fit the Pipeline – Call the 'fit()' method on the pipeline object with the training dataset; this executes the estimators to produce a trained 'PipelineModel'./n6. Transform Data – Use the resulting PipelineModel to call 'transform()' on new or test datasets, applying all preprocessing steps and the model prediction automatically./n7. Save and Load – The entire pipeline definition and trained model can be saved to disk and reloaded later, simplifying deployment and ensuring the exact same processing steps are applied in production./nThis process encapsulates the entire data science path from raw data to actionable predictions in a single, manageable entity.",
    "model-persistence": "Model Persistence in Spark ML refers to the ability to save and load machine learning models and pipelines, enabling reuse and deployment in production environments./n1. Serialization – Spark ML supports saving models and pipelines to disk by serializing the metadata, parameters, and learned data (like weights) into a storable format./n2. Save Functionality – The 'save()' method allows users to persist a trained model or a pipeline to a specified path in a distributed file system (e.g., HDFS, S3) or local storage./n3. Load Functionality – The 'load()' method allows users to read a saved model back into memory, restoring its state exactly as it was when saved./n4. Pipeline Persistence – Not just individual models, but entire pipelines (including feature engineering steps) can be saved, ensuring that the exact same preprocessing logic is applied during inference./n5. Cross-Language Compatibility – Models saved in one language (e.g., Scala) can often be loaded and used in another (e.g., Python), facilitating collaboration across different teams./n6. Versioning – Persistence enables model versioning, where different iterations of a model can be saved with unique paths or identifiers for tracking and rollback purposes./n7. Deployment – This is a critical step for MLOps, as it allows a model trained in a batch environment to be moved to a serving environment (e.g., a real-time web application or streaming job) without retraining./nModel persistence bridges the gap between experimental modeling and operationalizing machine learning solutions.",
    "spark-streaming-data": "Spark Streaming Data processing involves handling real-time data streams using micro-batch architecture to provide scalable and fault-tolerant analytics./n1. Micro-batching – Spark Streaming divides the continuous input data stream into small, discrete batches (micro-batches), which are then processed by the Spark engine as RDDs./n2. DStream Abstraction – The core abstraction is the Discretized Stream (DStream), representing a continuous stream of data fundamentally organized as a sequence of RDDs over time./n3. Diverse Sources – It enables ingestion of data from various sources such as Kafka, Flume, Kinesis, TCP sockets, or directory monitoring./n4. Unified Processing – Streaming data can be processed using the same high-level functions (map, reduce, join, window) used for batch processing, allowing code reuse./n5. Fault Tolerance – Leverages Spark's lineage-based fault tolerance mechanisms; if a node fails, lost data partitions can be recomputed from the source data./n6. Windowed Operations – Supports windowed computations, allowing operations over a sliding window of data (e.g., counting events over the last 10 minutes, updated every minute)./n7. Structured Streaming – A newer, more advanced API built on Spark SQL that treats live data as an unbounded table, enabling cleaner event-time processing and easier integration with static data./nSpark Streaming bridges the gap between batch and real-time processing, making it easier to build robust streaming applications.",
    "spark-streaming-working": "Spark Streaming works on a micro-batch architecture that treats live data streams as a series of small, static datasets (batches) for processing./n1. Data Ingestion – Continuous data flows into the system from sources like Kafka, Flume, or TCP sockets and is received by Receivers running on worker nodes./n2. Discretization – The continuous stream is chopped into small time intervals (e.g., 1 second or 5 seconds) called batches./n3. RDD Generation – For each time interval, Spark Streaming creates a Resilient Distributed Dataset (RDD) containing the data received during that period./n4. Processing Engine – These RDDs are processed by the core Spark engine using standard operations (map, reduce, join) or advanced machine learning algorithms./n5. Task Distribution – The Driver program divides the processing into tasks and schedules them across the Executors on worker nodes for parallel execution./n6. Output Generation – The processed results for each batch are pushed to external systems like HDFS, databases, or live dashboards./n7. Checkpointing & Logs – To ensure fault tolerance, metadata and data are periodically checkpointed, and Write-Ahead Logs (WAL) are used to prevent data loss in case of failure./nThis workflow allows Spark to apply its powerful batch processing engine to real-time data streams with high throughput and fault tolerance.",
    "kafka": "Apache Kafka is a distributed event streaming platform used for high-performance data pipelines, streaming analytics, and data integration./n1. Publish-Subscribe Model – Kafka works on a pub-sub model where producers publish messages to topics, and consumers subscribe to those topics to read the messages./n2. Topics and Partitions – Data is organized into 'Topics'. Each topic is divided into 'Partitions' to allow parallel processing and high scalability across multiple servers./n3. Distributed Logs – Kafka stores data as an immutable commit log on disk, ensuring durability and allowing consumers to read data at their own pace (including replaying old data)./n4. High Throughput – Designed to handle trillions of events per day with low latency, making it ideal for real-time applications like tracking, logging, and metrics./n5. Brokers and Clusters – A Kafka cluster consists of multiple servers called 'Brokers'. They manage the storage and replication of partitions to ensure fault tolerance./n6. Zookeeper/KRaft – Traditionally uses Zookeeper for cluster management (metadata, leader election), though newer versions are moving to KRaft mode (Kafka Raft) to remove this dependency./n7. Integration – Acts as a central nervous system for data, easily connecting with diverse systems like Hadoop, Spark, database connectors, and microservices./nKafka provides the backbone for real-time data streaming architectures, ensuring data is reliably moved between systems.",
    "kafka-working": "Kafka works as a distributed streaming platform by using a publish-subscribe model and maintaining a fault-tolerant, scalable, and durable log of events./n1. Producers – Applications or services that send (publish) messages (records) to Kafka topics. Each record consists of a key, a value, and a timestamp./n2. Brokers – Kafka servers that form the Kafka cluster. Brokers store data, handle requests from producers and consumers, and replicate partitions for fault tolerance./n3. Topics – Logical categories or feeds to which records are published. Topics are partitioned, and each partition is an ordered, immutable sequence of records that is continually appended to./n4. Partitions – A topic is divided into one or more partitions. Data written to a topic is appended to one of its partitions based on a hash of the key (or round-robin if no key)./n5. Consumers – Applications or services that read (subscribe to) messages from Kafka topics. Consumers read data from partitions in order and track their progress using offsets./n6. Consumer Groups – Multiple consumers can form a consumer group to share the workload of reading from topic partitions, allowing parallel consumption and scalability./n7. Replication – Each partition has a configurable number of replicas (copies) across different brokers to ensure data durability and high availability in case of broker failure./nThis architecture enables Kafka to provide high-throughput, low-latency, and fault-tolerant data streaming for real-time applications.",
    "kafka-commands": "Kafka Commands are command-line utilities used for managing and interacting with Kafka clusters, topics, producers, and consumers./n1. Topic Management – Commands like 'kafka-topics.sh --create', '--list', '--describe', and '--delete' are used to manage Kafka topics (creation, listing, viewing details, and deletion)./n2. Producer – The 'kafka-console-producer.sh' utility allows sending messages from the command line to a specified Kafka topic for quick testing or manual input./n3. Consumer – The 'kafka-console-consumer.sh' utility is used to read messages from Kafka topics, with options to specify a consumer group, read from the beginning, or read only new messages./n4. Consumer Group Management – 'kafka-consumer-groups.sh --list', '--describe', and '--reset-offsets' are used to view consumer groups, inspect their offset progress, and reset offsets./n5. Broker Information – 'kafka-broker-api-versions.sh' provides information about the API versions supported by brokers in the cluster./n6. Replication Factor and Partitions – Topic creation commands allow specifying the number of partitions and the replication factor for fault tolerance and parallelism./n7. Command Structure – Most Kafka commands follow a similar structure: 'kafka-[tool].sh --bootstrap-server [broker-list] [action] [options]'./nThese command-line tools are essential for administration, debugging, and basic interaction with a Kafka streaming environment.",
    "hadoop-commands": [
        "hadoop version",
        "hdfs dfs -ls -R -h",
        "hdfs dfs -mkdir -p",
        "hdfs dfs -cp",
        "hdfs dfs -put / -copyFromLocal",
        "hdfs dfs -get / -copyToLocal",
        "hdfs dfs -rm",
        "hdfs dfs -rmdir",
        "hdfs dfs -mv",
        "hdfs dfs -cat file.txt | less report.txt | head -10",
        "hdfs dfs -chmod 444",
        "hdfs dfs -du -h ",
        "hdfs dfs -help mkdir"
    ],
    "spark-commands": [
        "sc.parallelize(List(1, 2, 3, 4, 5))",
        "sc.textFile(\"/path/to/file.txt\")",
        "rdd.collect()",
        "rdd.count()",
        "rdd.first()",
        "rdd.filter(x => x % 2 == 0)     // even numbers",
        "rdd.map(x => x * x)   // square",
        "rdd.flatMap(line => line.split(\" \"))",
        "rdd.reduce((a, b) => a + b)",
        "pairs.reduceByKey((a, b) => a + b)",
        "rdd.groupByKey()",
        "rdd.distinct()",
        "rdd.sum()",
        "rdd.max()",
        "rdd.min()",
        "rdd.mean()",
        "rdd.stdev()",
        "rdd.variance()",
        "spark.read.json(\"/path/to/file.json\")",
        "df.select(\"name\",\"salary\")",
        "df.filter(df.salary > 50000)",
        "filtered.count()"
    ],
    "hive-commands":[
        "CREATE DATABASE anotherDB;",
        "CREATE TABLE employee_details (emp_id INT,name STRING,salary INT);",
        "CREATE EXTERNAL TABLE employee_details (emp_id INT,name STRING,salary INT);",
        "PARTITIONED BY (region STRING)",
        "ROW FORMAT DELIMITED",
        "FIELDS TERMINATED BY ','",
        "LOCATION '/user/hive/products'",
        "LOAD DATA LOCAL INPATH '/path/to/ordersfile'",
        "INTO TABLE anotherDB.orders1;"
    ],
    "mongodb-commands": [
        "db.createCollection(\"orders\")",
        "db.orders.insertOne({order_id: 1, order_status: \"CLOSED\"})",
        "db.orders.insertMany([{ order_id: 1, order_status: \"CLOSED\" }, { order_id: 2, order_status: \"OPEN\" }])",
        "db.orders.find()",
        "db.customer_data.find({ city: \"Delhi\" })",
        "db.users.find({ age: { $gte: 34 } })",
        "db.orders.find({order_status: { $in: [\"COMPLETE\", \"CLOSED\"] }})",
        "db.orders.countDocuments({order_status: { $in: [\"COMPLETE\", \"CLOSED\"] }})",
        "db.customer_data.updateMany({ city: \"Bombay\" },{ $set: { city: \"Mumbai\" } })",
        "db.customer_data.deleteMany({ name: \"John\" })",
        "use mydb"
    ]
}